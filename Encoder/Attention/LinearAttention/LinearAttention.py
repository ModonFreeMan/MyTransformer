import numpy as np

np.random.seed(0)

# -----------------------------
# 1. 基本维度设定
# -----------------------------
n = 4  # 序列长度
d = 3  # Q/K 特征维度
d_v = 2  # V 的维度

# -----------------------------
# 2. 构造 Q, K, V
# -----------------------------
Q = np.random.randn(n, d)  # (n, d)
K = np.random.randn(n, d)  # (n, d)
V = np.random.randn(n, d_v)  # (n, d_v)


# ------------------------------
# 关于特征映射 φ（phi）的说明
# ------------------------------
# 在 standard attention 中，权重来自：
#     weight_ij ∝ exp(Q_i^T K_j)   （随后再做 softmax 归一化）
# 这种形式要求显式计算 n×n 的注意力矩阵，时间和显存复杂度是 O(n^2)。
#
# 在 linear attention 中，我们放弃精确的 softmax 权重，
# 改用一种“可拆分”的近似形式，使得权重可以写成：
#     weight_ij ≈ φ(Q_i)^T φ(K_j)
# 这样 attention 聚合就能改写为：
#     O_i ≈ φ(Q_i)^T @ (sum_j φ(K_j) ⊗ V_j)
# 从而避免显式构造 n×n 的注意力矩阵，将复杂度降为 O(n·d)。
#
# φ 并不是 attention 的原始组成部分，而是为了线性化计算
# 人为引入的近似工具，因此 φ 的选择是一个工程权衡问题。
#
# 选择 φ 时需要满足的基本原则：
# 1. 输出必须非负：
#    原始 attention 权重 exp(Q_i^T K_j) 始终为正，
#    若 φ 产生负值会破坏“注意力权重”的直觉含义。
#
# 2. 数值稳定：
#    φ(K_j) 会在序列维度上被累加，
#    若 φ 输出过大或变化过激，长序列下容易数值爆炸或退化。
#
# 3. 保留相似性信息：
#    φ 不能把不同的 Q/K 映射成几乎相同的值，
#    否则 attention 会退化为对所有 token 的平均。
#
# 常见、稳定、工程上可接受的选择包括：
#     φ(x) = relu(x) + 1
#     φ(x) = elu(x) + 1
# 它们不能精确还原 softmax attention，
# 但在换取线性复杂度的同时能保持基本的注意力行为。
#
# 若需要更高精度的 softmax 近似，需要使用更复杂的随机映射方法
# （如 Performer / FAVOR+），代价是实现复杂度和数值控制成本。
# ------------------------------


# -----------------------------
# 3. 定义 feature map φ
#    这里用 elu + 1，保证非负
# -----------------------------
def phi(x):
    return np.maximum(0, x) + 1.0  # 简化版 elu+1


Q_phi = phi(Q)  # (n, d)
K_phi = phi(K)  # (n, d)

# =====================================================
# Part A：显式 attention（用于对照理解）
# =====================================================

# 显式构造 attention 权重矩阵 A
# A[i, j] = φ(q_i)^T φ(k_j)
A = Q_phi @ K_phi.T  # (n, n)

# 显式 attention 输出
O_explicit = A @ V  # (n, d_v)

# =====================================================
# Part B：Linear Attention（不构造 A）
# =====================================================

# 关键中间量：
# S = sum_j φ(k_j) ⊗ V_j
# 等价于：K_phi.T @ V
S = K_phi.T @ V  # (d, d_v)

# 对每个 i：
# O_i = φ(q_i)^T @ S
O_linear = Q_phi @ S  # (n, d_v)

# =====================================================
# 4. 打印结果
# =====================================================

print("显式 Attention 输出：")
print(O_explicit)

print("\nLinear Attention 输出：")
print(O_linear)

print("\n两者是否数值相等（允许微小误差）：")
print(np.allclose(O_explicit, O_linear))
